{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10a07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import calendar\n",
    "import pandas as pd\n",
    "import threading\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a8244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"bucket/dataset_customs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af78c2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bucket/dataset_customs/export', 'bucket/dataset_customs/import']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_ex_dir = [os.path.join(base,x) for x in os.listdir(base) if os.path.isdir(os.path.join(base,x))]\n",
    "im_ex_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d27584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_file_names(file_paths):\n",
    "    groups = {}\n",
    "    file_names = [(fp.split('/')[-1].split('-')[0], fp) for fp in file_paths]\n",
    "    for year, f_path in file_names:\n",
    "        if year in groups:\n",
    "            groups[year].append(f_path)\n",
    "        else:\n",
    "            groups[year] = [f_path]\n",
    "    for elm in groups:\n",
    "        groups[elm].sort()\n",
    "    sort_groups = sorted(groups.items(), key=lambda key: key[0])\n",
    "    return {elm[0]:elm[1] for elm in sort_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3ee42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [elm for elm in calendar.month_abbr]\n",
    "months_dict = { elm:months.index(elm) for elm in months}\n",
    "months_dict['Sept'] = 9\n",
    "def month2int(month):\n",
    "    return months_dict[month]\n",
    "\n",
    "def convert2com(name):\n",
    "    month_name, year = name.replace('(',' ').replace(')','').split(' ')[-2:]\n",
    "    month = month2int(month_name)\n",
    "    return \"{}-{:02d}\".format(year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be0e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_out = \"bucket/merge_dataset_customs\"\n",
    "if not os.path.exists(base_out):\n",
    "    os.mkdir(base_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e6b038",
   "metadata": {},
   "source": [
    "# Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6facd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(key, data):\n",
    "    sub_outpath = os.path.join(base_out, key)\n",
    "    if not os.path.exists(sub_outpath):\n",
    "        os.makedirs(sub_outpath)\n",
    "    merge_df_name = \"{}.csv\".format(data[0].split('/')[-2])\n",
    "    df = None\n",
    "    for index, f_path in enumerate(data):\n",
    "        df_tmp = pd.read_csv(f_path)\n",
    "        columns_acc = list(df_tmp.columns)[1:3]\n",
    "        df_tmp = df_tmp[columns_acc]\n",
    "        df_tmp = df_tmp.rename(columns={columns_acc[-1]:convert2com(columns_acc[-1])})\n",
    "        if index==0:\n",
    "            df = df_tmp\n",
    "        else:\n",
    "            df = pd.merge(df, df_tmp, how='outer')\n",
    "    df.to_csv(os.path.join(sub_outpath, merge_df_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab40915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = []\n",
    "for data_type in im_ex_dir:\n",
    "    category_paths = [os.path.join(data_type, x) for x in os.listdir(data_type) if os.path.isdir(os.path.join(data_type, x))]\n",
    "    category_paths.sort()\n",
    "    for category_path in tqdm(category_paths):\n",
    "        files_path = [os.path.join(category_path, x) for x in os.listdir(category_path) if os.path.isfile(os.path.join(category_path, x))]\n",
    "        group_file = group_file_names(files_path)\n",
    "        for group in group_file:\n",
    "            x = threading.Thread(target=merge_files, args=(group, group_file[group]))\n",
    "            t.append(x)\n",
    "            x.start()\n",
    "        for thread in t:\n",
    "            thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfd67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5068e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''single thread'''\n",
    "# for data_type in im_ex_dir:\n",
    "#     category_paths = [os.path.join(data_type, x) for x in os.listdir(data_type) if os.path.isdir(os.path.join(data_type, x))]\n",
    "#     category_paths.sort()\n",
    "#     for category_path in tqdm(category_paths):\n",
    "#         files_path = [os.path.join(category_path, x) for x in os.listdir(category_path) if os.path.isfile(os.path.join(category_path, x))]\n",
    "#         group_file = group_file_names(files_path)\n",
    "#         for key, data in group_file.items():\n",
    "#             sub_outpath = os.path.join(base_out, key)\n",
    "#             if not os.path.exists(sub_outpath):\n",
    "#                 os.makedirs(sub_outpath)\n",
    "#             merge_df_name = \"{}.csv\".format(data[0].split('/')[-2])\n",
    "#             df = None\n",
    "#             for index, f_path in enumerate(data):\n",
    "#                 df_tmp = pd.read_csv(f_path)\n",
    "#                 columns_acc = list(df_tmp.columns)[1:3]\n",
    "#                 df_tmp = df_tmp[columns_acc]\n",
    "#                 df_tmp = df_tmp.rename(columns={columns_acc[-1]:convert2com(columns_acc[-1])})\n",
    "#                 if index==0:\n",
    "#                     df = df_tmp\n",
    "#                 else:\n",
    "#                     df = pd.merge(df, df_tmp, how='outer')\n",
    "#             df.to_csv(os.path.join(sub_outpath, merge_df_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12898d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b25c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3c81e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bucket/merge_dataset_customs/2001',\n",
       " 'bucket/merge_dataset_customs/2002',\n",
       " 'bucket/merge_dataset_customs/2003',\n",
       " 'bucket/merge_dataset_customs/2004',\n",
       " 'bucket/merge_dataset_customs/2005',\n",
       " 'bucket/merge_dataset_customs/2006',\n",
       " 'bucket/merge_dataset_customs/2007',\n",
       " 'bucket/merge_dataset_customs/2008',\n",
       " 'bucket/merge_dataset_customs/2009',\n",
       " 'bucket/merge_dataset_customs/2010',\n",
       " 'bucket/merge_dataset_customs/2011',\n",
       " 'bucket/merge_dataset_customs/2012',\n",
       " 'bucket/merge_dataset_customs/2013',\n",
       " 'bucket/merge_dataset_customs/2014',\n",
       " 'bucket/merge_dataset_customs/2015',\n",
       " 'bucket/merge_dataset_customs/2016',\n",
       " 'bucket/merge_dataset_customs/2017',\n",
       " 'bucket/merge_dataset_customs/2018',\n",
       " 'bucket/merge_dataset_customs/2019',\n",
       " 'bucket/merge_dataset_customs/2020',\n",
       " 'bucket/merge_dataset_customs/2021']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dirs = [os.path.join(base_out,x) for x in os.listdir(base_out) if os.path.isdir(os.path.join(base_out,x))]\n",
    "out_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c219f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files after merge : 26,334\n"
     ]
    }
   ],
   "source": [
    "all_files_path = []\n",
    "for out_dir in out_dirs:\n",
    "    all_files_path += [os.path.join(out_dir, x) for x in os.listdir(out_dir) if os.path.isfile(os.path.join(out_dir, x))]\n",
    "print(\"Total files after merge : {:,}\".format(len(all_files_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbfa181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
